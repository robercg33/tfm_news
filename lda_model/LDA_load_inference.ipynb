{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de como usar el modelo LDA. Se necesitan las siguientes librerias:"
      ],
      "metadata": {
        "id": "yz38vDF3pmg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk gensim joblib spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0DRc1Pbp9Oq",
        "outputId": "faf58609-6ba2-4c1b-cdc6-7729191b945d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los imports necesarios:\n"
      ],
      "metadata": {
        "id": "jUKwn1FsqHIZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrjoGpFipfEP",
        "outputId": "d887c2b7-fa86-421f-9b67-2e4388baca46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import json\n",
        "import pickle\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaMulticore\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Download NLTK resources if needed\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y se tiene que crear la siguiente clase:"
      ],
      "metadata": {
        "id": "6HIDJTQ-qQHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LDAModelWrapper:\n",
        "    def __init__(self, lda_model, dictionary, config):\n",
        "        \"\"\"\n",
        "        Initializes the LDAModelWrapper with a trained LDA model, dictionary, and configuration.\n",
        "\n",
        "        Parameters:\n",
        "        - lda_model: A trained Gensim LdaMulticore model.\n",
        "        - dictionary: A Gensim Dictionary object used for the model.\n",
        "        - config: Configuration dictionary or JSON loaded parameters.\n",
        "        \"\"\"\n",
        "        self.lda_model = lda_model\n",
        "        self.dictionary = dictionary\n",
        "        self.config = config\n",
        "        self.stemming = config.get('stemming', False)\n",
        "        self.preprocess_library = config.get('preprocess_library', 'nltk')\n",
        "        self.no_above = config.get('no_above', 0.5)\n",
        "        self.no_below = config.get('no_below', 5)\n",
        "        self.topics_dict = config.get('topics_dict', {})  # Load the topic labels dictionary from config\n",
        "\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocesses the input text based on configuration settings.\n",
        "\n",
        "        Parameters:\n",
        "        - text: A string representing the text to preprocess.\n",
        "\n",
        "        Returns:\n",
        "        - A list of processed tokens.\n",
        "        \"\"\"\n",
        "        # Convert text to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove punctuation and numbers\n",
        "        text = re.sub(r'[\\d]', '', text)\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        if self.preprocess_library == \"nltk\":\n",
        "            # Tokenize the text\n",
        "            tokens = word_tokenize(text)\n",
        "\n",
        "            # Remove stopwords using NLTK\n",
        "            stop_words_nltk = set(stopwords.words('english'))\n",
        "            tokens = [word for word in tokens if word not in stop_words_nltk]\n",
        "\n",
        "            # Apply lemmatization\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        elif self.preprocess_library == \"spacy\":\n",
        "            # Process the text with spaCy\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Filter and lemmatize tokens\n",
        "            tokens = [token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS and not token.is_punct and not token.is_space]\n",
        "        else:\n",
        "            # If invalid preprocess library, raise an error\n",
        "            raise ValueError(\"Invalid preprocess library, must be -> ['nltk', 'spacy']\")\n",
        "\n",
        "        # Apply stemming if required\n",
        "        if self.stemming:\n",
        "            stemmer = SnowballStemmer(\"english\")\n",
        "            tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def get_topic_scores(self, text, threshold=0.1, verbose=False):\n",
        "        \"\"\"\n",
        "        Gets the topic distribution and scores for an unseen document.\n",
        "\n",
        "        Parameters:\n",
        "        - text: The unseen document as a string.\n",
        "        - threshold: Minimum score threshold to display topics.\n",
        "\n",
        "        Returns:\n",
        "        - A list of tuples containing topic indices and their scores.\n",
        "        \"\"\"\n",
        "        # Preprocess the unseen document\n",
        "        tokens = self.preprocess_text(text)\n",
        "        bow_vector = self.dictionary.doc2bow(tokens)\n",
        "\n",
        "        # Get topic distribution for the document\n",
        "        topic_scores = sorted(self.lda_model[bow_vector], key=lambda tup: -tup[1])\n",
        "\n",
        "        # Filter topics based on the threshold and map to human-readable labels\n",
        "        filtered_topics = [(self.topics_dict.get(index, f\"Topic {index}\"), score)\n",
        "                           for index, score in topic_scores if score >= threshold]\n",
        "\n",
        "        # Print the topics and scores IF verbose flag added\n",
        "        if verbose:\n",
        "          for label, score in filtered_topics:\n",
        "              print(f\"Score: {score}\\nTopic: {label}\\n\")\n",
        "\n",
        "        return filtered_topics\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Saves the LDA model and dictionary to a file.\n",
        "\n",
        "        Parameters:\n",
        "        - filepath: The path to save the model file.\n",
        "        \"\"\"\n",
        "        with open(filepath, 'wb') as file:\n",
        "            pickle.dump({'lda_model': self.lda_model, 'dictionary': self.dictionary, 'config': self.config}, file)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filepath):\n",
        "        \"\"\"\n",
        "        Loads the LDA model and dictionary from a file.\n",
        "\n",
        "        Parameters:\n",
        "        - filepath: The path to the model file to load.\n",
        "\n",
        "        Returns:\n",
        "        - An instance of LDAModelWrapper.\n",
        "        \"\"\"\n",
        "        with open(filepath, 'rb') as file:\n",
        "            data = pickle.load(file)\n",
        "        print(f\"Model loaded from {filepath}\")\n",
        "        return cls(data['lda_model'], data['dictionary'], data['config'])"
      ],
      "metadata": {
        "id": "3FfgVnayqR9l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, a partir del archivo *lda_model.pkl* se carga en la clase creada."
      ],
      "metadata": {
        "id": "yd_X8uLDqYT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"lda_model.pkl\" #Cambiar con la ruta del archivo\n",
        "\n",
        "#Generar una instancia de la clase a partir del archivo con el metodo load_model de la clase\n",
        "lda_wraper = LDAModelWrapper.load_model(filepath=file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr15VdgEqeQ4",
        "outputId": "ec49358f-b013-4644-d074-23811717bd03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from lda_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, ya se puede hacer inferencia con ese modelo de la siguiente forma:"
      ],
      "metadata": {
        "id": "DuVg9_SNraMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_a_inferir_tema = \"This is an example of text. Let's say I want to talk about university, college, research or investigation. For example, in my university class there are teachers who impart lectures about machine learning.\"\n",
        "\n",
        "lda_wraper.get_topic_scores(text=texto_a_inferir_tema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-X7LW_ErXv5",
        "outputId": "ab1351c2-9fce-4d52-e126-9f4307087fc9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Education', 0.7231515), ('Justice', 0.12359578), ('Economy', 0.11954031)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}